from typing import List, Dict, Any, Optional, AsyncGenerator, Union
import json
import asyncio
import time
import logging

try:
    from vllm import LLM, SamplingParams
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False
    LLM = None
    SamplingParams = None
    AsyncEngineArgs = None
    AsyncLLMEngine = None
    logging.warning("vLLM is not available. Install it with: pip install vllm")

from app.models.schemas import ChatResponse, ChatChoice, Message, UsageInfo
from app.services.base_llm_handler import BaseLLMHandler
from app.core.config import settings
from app.utils import convert_to_dict_messages

logger = logging.getLogger(__name__)


class VLLMHandler(BaseLLMHandler):
    """Handler –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å vLLM."""
    
    _instance = None

    def __init__(self):
        super().__init__()
        if not VLLM_AVAILABLE:
            error_msg = "vLLM is not available. Please install it with: pip install vllm"
            logger.error(error_msg)
            raise ImportError(error_msg)
        
        self.engine: Optional[AsyncLLMEngine] = None
        self.model_path = settings.model.path
        self.model_name = settings.model.name
        self.max_model_len = settings.model.ctx_size
        self.tensor_parallel_size = getattr(settings.model, 'tensor_parallel_size', 1)
        self.gpu_memory_utilization = getattr(settings.model, 'gpu_memory_utilization', 0.9)
        self.trust_remote_code = getattr(settings.model, 'trust_remote_code', False)
        self.quantization = getattr(settings.model, 'quantization', None)

    @classmethod
    def get_instance(cls):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Å–∏–Ω–≥–ª—Ç–æ–Ω–∞."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ vLLM."""
        if self.is_initialized:
            return

        if not VLLM_AVAILABLE:
            raise ImportError("vLLM is not available. Please install it with: pip install vllm")

        try:
            print("=" * 80)
            print("üöÄ INITIALIZING VLLM BACKEND")
            print("=" * 80)
            print(f"üìÅ Model path: {self.model_path}")
            print(f"üìù Model name: {self.model_name}")
            print(f"üíæ Max model length: {self.max_model_len} tokens")
            print(f"üîÄ Tensor parallel size: {self.tensor_parallel_size} GPU(s)")
            print(f"üíø GPU memory utilization: {self.gpu_memory_utilization * 100:.0f}%")
            print(f"üîê Trust remote code: {self.trust_remote_code}")
            if self.quantization:
                print(f"‚ö° Quantization: {self.quantization}")
            print("-" * 80)
            print("‚è≥ Loading vLLM model (this may take a while)...")
            
            logger.info(f"Loading vLLM model from {self.model_path}")
            
            # –°–æ–∑–¥–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞
            # –í–ê–ñ–ù–û: vLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CUDA), –ø–∞—Ä–∞–º–µ—Ç—Ä device –Ω–µ –Ω—É–∂–µ–Ω
            engine_args_dict = {
                "model": self.model_path,
                "max_model_len": self.max_model_len,
                "tensor_parallel_size": self.tensor_parallel_size,
                "gpu_memory_utilization": self.gpu_memory_utilization,
                "trust_remote_code": self.trust_remote_code,
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä quantization, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if self.quantization:
                engine_args_dict["quantization"] = self.quantization
            
            engine_args = AsyncEngineArgs(**engine_args_dict)
            
            # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            
            self.is_initialized = True
            print("‚úÖ vLLM model loaded successfully!")
            print("=" * 80)
            logger.info("vLLM model loaded successfully")
        except Exception as e:
            print("‚ùå Failed to load vLLM model!")
            print(f"   Error: {str(e)}")
            print("=" * 80)
            logger.error(f"Failed to load vLLM model: {str(e)}")
            raise

    def is_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å."""
        return self.is_initialized and self.engine is not None

    def _format_messages_for_vllm(self, messages: List[Message]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è vLLM (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –ø—Ä–æ–º–ø—Ç).
        
        vLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç chat template –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω.
        –î–ª—è –º–æ–¥–µ–ª–µ–π Qwen –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å —Ç–æ–∫–µ–Ω–∞–º–∏ <|im_start|> –∏ <|im_end|>.
        –î–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        msg_dicts = []
        for msg in messages:
            if isinstance(msg, Message):
                msg_dicts.append({"role": msg.role, "content": msg.content})
            else:
                msg_dicts.append(msg)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å Qwen (–ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—É—Ç–∏)
        is_qwen = "qwen" in self.model_name.lower() or "qwen" in self.model_path.lower()
        
        if is_qwen:
            # –§–æ—Ä–º–∞—Ç –¥–ª—è Qwen –º–æ–¥–µ–ª–µ–π
            formatted_parts = []
            for msg in msg_dicts:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                # Qwen –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                formatted_parts.append(f"<|im_start|>{role}\n{content}<|im_end|>\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            formatted_parts.append("<|im_start|>assistant\n")
            return "".join(formatted_parts)
        else:
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            # vLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç chat template, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
            formatted_parts = []
            for msg in msg_dicts:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "system":
                    formatted_parts.append(f"System: {content}\n")
                elif role == "user":
                    formatted_parts.append(f"User: {content}\n")
                elif role == "assistant":
                    formatted_parts.append(f"Assistant: {content}\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            formatted_parts.append("Assistant:")
            return "".join(formatted_parts)

    async def generate_response(
        self,
        messages: List[Message],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Union[ChatResponse, AsyncGenerator[str, None]]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ vLLM."""
        if not self.is_loaded():
            raise ValueError("Model not loaded")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        temperature = temperature or settings.generation.default_temperature
        max_tokens = max_tokens or settings.generation.default_max_tokens

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç
        prompt = self._format_messages_for_vllm(messages)

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
        )

        start_time = time.time()

        if stream:
            # –ü–æ—Ç–æ–∫–æ–≤—ã–π —Ä–µ–∂–∏–º
            async def stream_generator():
                try:
                    import uuid
                    request_id = str(uuid.uuid4())
                    prev_text = ""
                    
                    async for request_output in self.engine.generate(prompt, sampling_params, request_id):
                        # –ü–æ–ª—É—á–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        for output in request_output.outputs:
                            current_text = output.text
                            # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç (delta)
                            new_text = current_text[len(prev_text):]
                            prev_text = current_text
                            
                            if new_text:
                                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º chunk –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI SSE
                                chunk = {
                                    "id": f"chatcmpl-{request_id}",
                                    "object": "chat.completion.chunk",
                                    "created": int(time.time()),
                                    "model": self.model_name,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": new_text},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(chunk)}\n\n"
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
                            if output.finish_reason is not None:
                                # –§–∏–Ω–∞–ª—å–Ω—ã–π chunk
                                final_chunk = {
                                    "id": f"chatcmpl-{request_id}",
                                    "object": "chat.completion.chunk",
                                    "created": int(time.time()),
                                    "model": self.model_name,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {},
                                        "finish_reason": output.finish_reason
                                    }]
                                }
                                yield f"data: {json.dumps(final_chunk)}\n\n"
                except Exception as e:
                    logger.error(f"Error in stream generation: {str(e)}")
                    raise
                finally:
                    yield "data: [DONE]\n\n"

            processing_time = time.time() - start_time
            logger.info(f"Stream response generated in {processing_time:.2f}s")
            return stream_generator()
        else:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            import uuid
            request_id = str(uuid.uuid4())
            final_output = None
            
            async for request_output in self.engine.generate(prompt, sampling_params, request_id):
                final_output = request_output
            
            if final_output is None or not final_output.outputs:
                raise ValueError("No output generated")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            output = final_output.outputs[0]
            generated_text = output.text
            finish_reason = output.finish_reason or "stop"

            processing_time = time.time() - start_time
            logger.info(f"Response generated in {processing_time:.2f}s")

            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
            prompt_tokens = 0
            completion_tokens = 0
            if hasattr(final_output, 'metrics'):
                metrics = final_output.metrics
                prompt_tokens = getattr(metrics, 'num_prompt_tokens', 0)
                completion_tokens = getattr(metrics, 'num_generated_tokens', 0)

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            return ChatResponse(
                id=f"chatcmpl-{request_id}",
                created=int(time.time()),
                model=self.model_name,
                choices=[
                    ChatChoice(
                        index=0,
                        message=Message(
                            role="assistant",
                            content=generated_text
                        ),
                        finish_reason=finish_reason
                    )
                ],
                usage=UsageInfo(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    total_tokens=prompt_tokens + completion_tokens
                )
            )

    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏."""
        if self.engine:
            # vLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–µ—Ä–µ–∑ AsyncLLMEngine
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —è–≤–Ω—É—é –æ—á–∏—Å—Ç–∫—É, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            pass
        self.engine = None
        self.is_initialized = False
        logger.info("vLLM model resources cleaned up")

