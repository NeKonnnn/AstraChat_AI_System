from llama_cpp import Llama
from typing import List, Dict, Any, Optional, Callable, AsyncGenerator, Union
import json
import asyncio
import time
import logging

from app.models.schemas import ChatResponse, ChatChoice, Message, UsageInfo
from app.utils import convert_to_dict_messages, convert_to_chat_completion_messages
from app.core.config import settings
from app.services.base_llm_handler import BaseLLMHandler

logger = logging.getLogger(__name__)


class LlamaHandler(BaseLLMHandler):
    _instance = None

    def __init__(self):
        self.model: Optional[Llama] = None
        self.model_path = settings.model.path
        self.model_name = settings.model.name
        self.n_ctx = settings.model.ctx_size
        self.n_gpu_layers = settings.model.gpu_layers
        self.verbose = settings.model.verbose
        self.is_initialized = False

    @classmethod
    def get_instance(cls):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Å–∏–Ω–≥–ª—Ç–æ–Ω–∞."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏."""
        if self.is_initialized:
            return

        try:
            print("=" * 80)
            print("üöÄ INITIALIZING LLAMA.CPP BACKEND")
            print("=" * 80)
            print(f"üìÅ Model path: {self.model_path}")
            print(f"üìù Model name: {self.model_name}")
            print(f"üíæ Context size: {self.n_ctx} tokens")
            print(f"üéÆ GPU layers: {self.n_gpu_layers}")
            print(f"üîä Verbose: {self.verbose}")
            print("-" * 80)
            print("‚è≥ Loading model...")
            
            logger.info(f"Loading model from {self.model_path}")
            loop = asyncio.get_event_loop()
            self.model = await loop.run_in_executor(
                None,
                lambda: Llama(
                    model_path=self.model_path,
                    n_threads=7,
                    n_threads_batch=7,
                    n_ctx=self.n_ctx,
                    n_gpu_layers=self.n_gpu_layers,
                    verbose=self.verbose
                )
            )
            self.is_initialized = True
            print("‚úÖ llama.cpp model loaded successfully!")
            print("=" * 80)
            logger.info("Model loaded successfully")
        except Exception as e:
            print("‚ùå Failed to load llama.cpp model!")
            print(f"   Error: {str(e)}")
            print("=" * 80)
            logger.error(f"Failed to load model: {str(e)}")
            raise

    async def _run_in_executor(self, func: Callable):
        """–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –≤ executor'–µ."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, func)

    def is_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å."""
        return self.is_initialized and self.model is not None

    async def _try_create_completion(self, messages: List[Message],
                                     temperature: float, max_tokens: int,
                                     stream: bool) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:
        """–ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —á–∞—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π."""

        # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è completion
        def create_completion( messages_formatter: Callable):
            formatted_messages = messages_formatter(messages)
            return self.model.create_chat_completion(
                messages=formatted_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )

        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä–µ–π
            return await self._run_in_executor(lambda: create_completion(convert_to_dict_messages))
        except TypeError as e:
            if "Expected type" in str(e) and "got 'list[dict" in str(e):
                # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ —Ç–∏–ø–∞, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã —Å–æ–æ–±—â–µ–Ω–∏–π
                logger.info("Falling back to typed messages for chat completion")
                return await self._run_in_executor(lambda: create_completion(convert_to_chat_completion_messages))
            else:
                raise e

    async def generate_response(self, messages: List[Message],
                                temperature: Optional[float] = None,
                                max_tokens: Optional[int] = None,
                                stream: bool = False) -> Union[ChatResponse, AsyncGenerator[str, None]]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –æ–±–∞ —Ä–µ–∂–∏–º–∞."""
        if not self.is_loaded():
            raise ValueError("Model not loaded")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        temperature = temperature or settings.generation.default_temperature
        max_tokens = max_tokens or settings.generation.default_max_tokens

        start_time = time.time()

        if stream:
            # –ü–æ—Ç–æ–∫–æ–≤—ã–π —Ä–µ–∂–∏–º - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
            stream_result = await self._try_create_completion(
                messages, temperature, max_tokens, stream=True
            )

            async def stream_generator():
                try:
                    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –ø–æ—Ç–æ–∫—É
                    while True:
                        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π chunk –∏–∑ –ø–æ—Ç–æ–∫–∞
                        chunk = await self._run_in_executor(lambda: next(stream_result, None))
                        if chunk is None:
                            break

                        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º chunk –≤ —Å—Ç—Ä–æ–∫—É SSE
                        yield f"data: {json.dumps(chunk)}\n\n"
                except StopIteration:
                    pass
                finally:
                    # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø–æ—Ç–æ–∫
                    yield "data: [DONE]\n\n"

            processing_time = time.time() - start_time
            logger.info(f"Stream response generated in {processing_time:.2f}s")
            return stream_generator()
        else:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            response = await self._try_create_completion(
                messages, temperature, max_tokens, stream=False
            )

            processing_time = time.time() - start_time
            logger.info(f"Response generated in {processing_time:.2f}s")

            return self._format_response(response)

    def _format_response(self, raw_response: Dict[str, Any]) -> ChatResponse:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI —Ñ–æ—Ä–º–∞—Ç."""
        choice = raw_response["choices"][0]
        message = choice["message"]

        return ChatResponse(
            id=f"chatcmpl-{int(time.time())}",
            created=int(time.time()),
            model=self.model_name,
            choices=[
                ChatChoice(
                    index=0,
                    message=Message(
                        role=message["role"],
                        content=message["content"]
                    ),
                    finish_reason=choice.get("finish_reason", "stop")
                )
            ],
            usage=UsageInfo(
                prompt_tokens=raw_response.get("usage", {}).get("prompt_tokens", 0),
                completion_tokens=raw_response.get("usage", {}).get("completion_tokens", 0),
                total_tokens=raw_response.get("usage", {}).get("total_tokens", 0)
            )
        )

    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏."""
        if self.model:
            del self.model
        self.model = None
        self.is_initialized = False
        logger.info("Model resources cleaned up")
