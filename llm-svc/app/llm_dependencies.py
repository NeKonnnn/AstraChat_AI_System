from app.services.llama_handler import LlamaHandler
from app.services.base_llm_handler import BaseLLMHandler
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM
_llm_handler = None

async def get_llm_handler() -> BaseLLMHandler:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    global _llm_handler
    if _llm_handler is None:
        backend = settings.model.backend.lower()
        
        print("=" * 80)
        print("üîß LLM BACKEND SELECTION")
        print("=" * 80)
        print(f"üìã Configured backend: {settings.model.backend}")
        
        if backend == "vllm":
            try:
                from app.services.vllm_handler import VLLMHandler
                print("‚úÖ Selected backend: vLLM")
                print("   - High performance inference engine")
                print("   - Optimized for GPU acceleration")
                print("   - Supports continuous batching")
                _llm_handler = VLLMHandler.get_instance()
            except ImportError as e:
                print("‚ùå vLLM import failed, falling back to llama.cpp")
                print(f"   Error: {str(e)}")
                logger.error(f"Failed to import VLLMHandler: {str(e)}")
                logger.warning("Falling back to llama.cpp backend")
                print("‚úÖ Selected backend: llama.cpp (fallback)")
                _llm_handler = LlamaHandler.get_instance()
        elif backend == "llama.cpp" or backend == "llamacpp":
            print("‚úÖ Selected backend: llama.cpp")
            print("   - GGUF model format support")
            print("   - CPU and GPU support")
            print("   - Memory efficient quantization")
            _llm_handler = LlamaHandler.get_instance()
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º llama.cpp
            print(f"‚ö†Ô∏è  Unknown backend '{backend}', falling back to llama.cpp")
            print("‚úÖ Selected backend: llama.cpp (default)")
            logger.warning(f"Unknown backend '{backend}', falling back to llama.cpp")
            _llm_handler = LlamaHandler.get_instance()
        
        print("=" * 80)
        
        try:
            await _llm_handler.initialize()
        except Exception as e:
            logger.error(f"Failed to initialize LLM handler: {str(e)}")
            # –ï—Å–ª–∏ vLLM –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–±—É–µ–º llama.cpp
            if backend == "vllm" and not isinstance(_llm_handler, LlamaHandler):
                print("=" * 80)
                print("‚ùå vLLM initialization failed, falling back to llama.cpp")
                print(f"   Error: {str(e)}")
                print("=" * 80)
                logger.warning("vLLM initialization failed, falling back to llama.cpp")
                _llm_handler = LlamaHandler.get_instance()
                await _llm_handler.initialize()
            else:
                raise
    return _llm_handler

# –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
async def get_llama_handler() -> BaseLLMHandler:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)."""
    return await get_llm_handler()

async def cleanup_llm_handler():
    """–û—á–∏—Å—Ç–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM."""
    global _llm_handler
    if _llm_handler is not None:
        await _llm_handler.cleanup()
        _llm_handler = None

# –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
async def cleanup_llama_handler():
    """–û—á–∏—Å—Ç–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ LLM (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)."""
    await cleanup_llm_handler()