import os
import tempfile
import asyncio
import logging
from io import BytesIO
import docx
import PyPDF2
import openpyxl
import pdfplumber
from typing import Optional, Dict, List, Any
from datetime import datetime
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL + pgvector
try:
    from backend.database.init_db import get_vector_repository, get_document_repository
    from backend.database.postgresql.models import Document as PGDocument, DocumentVector
    pgvector_available = True
except ImportError as e:
    print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: PostgreSQL –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
    print("DocumentProcessor –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —Ä–µ–∂–∏–º–µ fallback (–±–µ–∑ pgvector)")
    pgvector_available = False
    get_vector_repository = None
    get_document_repository = None
    PGDocument = None
    DocumentVector = None

class DocumentProcessor:
    def __init__(self):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º DocumentProcessor...")
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å pgvector
        self.documents = []  # –ö—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.doc_names = []
        self.embeddings = None
        self.vectorstore = None  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ —Ñ–ª–∞–≥, —á—Ç–æ pgvector –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        # –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PostgreSQL
        self.vector_repo = None
        self.document_repo = None
        # –ú–∞–ø–ø–∏–Ω–≥ filename -> document_id –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.filename_to_id: Dict[str, int] = {}
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        # {filename: {"confidence": float, "text_length": int, "file_type": str, "words": [{"word": str, "confidence": float}]}}
        self.confidence_data = {}
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        # {filename: {"path": file_path, "minio_object": object_name, "minio_bucket": bucket_name}}
        # –∏–ª–∏ {filename: file_path} –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.image_paths = {}
        # –ö—ç—à —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫–æ –≤—Å–µ–º —á–∞–Ω–∫–∞–º
        # {doc_name: [{"content": str, "chunk": int}, ...]} - –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ chunk
        self._doc_chunks_cache = {}

        logger.info("DocumentProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.init_embeddings()
        self.init_pgvector()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        status = self.get_pgvector_status()
        if status["available"] and status["initialized"]:
            logger.info(f"PGVECTOR –ì–û–¢–û–í –ö –†–ê–ë–û–¢–ï")
            logger.info(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ: {status['documents_count']}")
        elif status["available"]:
            logger.warning(f"PGVECTOR –î–û–°–¢–£–ü–ï–ù, –ù–û –ù–ï –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù")
            if status.get("error"):
                logger.warning(f"   –û—à–∏–±–∫–∞: {status['error']}")
        else:
            logger.warning(f"PGVECTOR –ù–ï–î–û–°–¢–£–ü–ï–ù")
        
    def init_embeddings(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        try:
            # –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ - —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º /app/models (–º–æ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ ./models)
            # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º backend/models (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
            model_name_local = "paraphrase-multilingual-MiniLM-L12-v2"
            
            # –í–∞—Ä–∏–∞–Ω—Ç 1: –ú–æ–¥–µ–ª—å –≤ /app/models (Docker)
            model_path_docker = os.path.join("/app/models", model_name_local)
            # –í–∞—Ä–∏–∞–Ω—Ç 2: –ú–æ–¥–µ–ª—å –≤ backend/models (–ª–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞)
            model_path_local = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                "models",
                model_name_local
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
            if os.path.exists(model_path_docker):
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (Docker): {model_path_docker}")
                model_name = model_path_docker
            elif os.path.exists(model_path_local):
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–ª–æ–∫–∞–ª—å–Ω–æ): {model_path_local}")
                model_name = model_path_local
            else:
                # Fallback –Ω–∞ Hugging Face Hub
                print("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ Hugging Face Hub...")
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            
            # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU, —Ç–∞–∫ –∫–∞–∫ CUDA –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –Ω–æ–≤—ã–µ GPU
            self.embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'}  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
            )
            print("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (CPU)")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
            import traceback
            traceback.print_exc()
            self.embeddings = None
    
    def get_pgvector_status(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ pgvector
        
        Returns:
            dict: –°—Ç–∞—Ç—É—Å pgvector —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        status = {
            "available": pgvector_available,
            "initialized": False,
            "repositories_ready": False,
            "vectorstore_active": False,
            "documents_count": 0,
            "vectors_count": 0,
            "error": None
        }
        
        if not pgvector_available:
            status["error"] = "–ú–æ–¥—É–ª–∏ PostgreSQL –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã"
            return status
        
        try:
            status["initialized"] = self.vector_repo is not None and self.document_repo is not None
            status["repositories_ready"] = status["initialized"]
            status["vectorstore_active"] = self.vectorstore is True
            status["documents_count"] = len(self.doc_names)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –ë–î
            if self.vector_repo:
                try:
                    # –≠—Ç–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤, –Ω–æ –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞ –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
                    status["vectors_count"] = "N/A (—Ç—Ä–µ–±—É–µ—Ç async –∑–∞–ø—Ä–æ—Å)"
                except:
                    pass
        except Exception as e:
            status["error"] = str(e)
        
        return status
    
    def init_pgvector(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ pgvector"""
        logger.info("=" * 60)
        logger.info("–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø PGVECTOR")
        logger.info("=" * 60)
        
        if not pgvector_available:
            logger.warning("PGVECTOR –ù–ï–î–û–°–¢–£–ü–ï–ù")
            logger.warning("–ú–æ–¥—É–ª–∏ PostgreSQL –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")
            logger.warning("DocumentProcessor –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ")
            logger.warning("–î–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PostgreSQL —Å pgvector")
            logger.info("=" * 60)
            return
        
        try:
            logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ pgvector...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
            try:
                self.vector_repo = get_vector_repository()
                logger.info("‚úÖ VectorRepository –ø–æ–ª—É—á–µ–Ω")
            except RuntimeError as e:
                error_msg = str(e)
                if "–Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω" in error_msg or "–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã" in error_msg:
                    logger.error("‚ùå PostgreSQL –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                    logger.error(f"   {error_msg}")
                    logger.error("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
                    logger.error("   1. PostgreSQL –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
                    logger.error("   2. –í—ã–∑–≤–∞–Ω init_postgresql() –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º DocumentProcessor")
                    logger.error("   3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ .env —Ñ–∞–π–ª–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
                    logger.info("=" * 60)
                    self.vector_repo = None
                    self.document_repo = None
                    self.vectorstore = None
                    return
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è VectorRepository: {error_msg}")
                    raise
            
            try:
                self.document_repo = get_document_repository()
                logger.info("‚úÖ DocumentRepository –ø–æ–ª—É—á–µ–Ω")
            except RuntimeError as e:
                error_msg = str(e)
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è DocumentRepository: {error_msg}")
                self.vector_repo = None
                self.document_repo = None
                self.vectorstore = None
                logger.info("=" * 60)
                return
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è pgvector
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è pgvector...")
            pgvector_extension_available = asyncio.run(self._check_pgvector_extension())
            
            if not pgvector_extension_available:
                logger.error("‚ùå PGVECTOR –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù –í POSTGRESQL")
                logger.error("   –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ 'vector' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
                logger.error("   –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ pgvector:")
                logger.error("   1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pgvector –≤ PostgreSQL (—Å–º. README/QUICK_START_POSTGRESQL_PGVECTOR.md)")
                logger.error("   2. –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Docker –æ–±—Ä–∞–∑ —Å pgvector: pgvector/pgvector:pg17")
                logger.error("   3. –ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: CREATE EXTENSION vector;")
                logger.warning("‚ö†Ô∏è  DocumentProcessor –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è")
                logger.info("=" * 60)
                self.vector_repo = None
                self.document_repo = None
                self.vectorstore = None
                return
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ pgvector...")
            is_working = asyncio.run(self._test_pgvector_connection())
            
            if is_working:
                self.vectorstore = True  # –§–ª–∞–≥, —á—Ç–æ pgvector –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                logger.info("‚úÖ PGVECTOR –†–ê–ë–û–¢–ê–ï–¢ –ö–û–†–†–ï–ö–¢–ù–û")
                logger.info("   - –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                logger.info("   - –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                logger.info("   - –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                logger.info("   - –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ—Å—Ç—É–ø–µ–Ω")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ë–î
                self._load_documents_from_db()
            else:
                logger.warning("PGVECTOR –ù–ï –†–ê–ë–û–¢–ê–ï–¢")
                logger.warning("   –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ –ø—Ä–æ—à–ª–∞")
                self.vector_repo = None
                self.document_repo = None
                self.vectorstore = None
            
            logger.info("=" * 60)
            
        except RuntimeError as e:
            error_msg = str(e)
            logger.error("–û–®–ò–ë–ö–ê –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò PGVECTOR")
            if "–Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω" in error_msg or "–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã" in error_msg:
                logger.error(f"   PostgreSQL –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {error_msg}")
                logger.error("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
                logger.error("   1. PostgreSQL –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
                logger.error("   2. –í—ã–∑–≤–∞–Ω init_postgresql() –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º DocumentProcessor")
                logger.error("   3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ .env —Ñ–∞–π–ª–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
            else:
                logger.error(f"   {error_msg}")
            logger.info("=" * 60)
            self.vector_repo = None
            self.document_repo = None
            self.vectorstore = None
        except Exception as e:
            error_msg = str(e)
            logger.error("–û–®–ò–ë–ö–ê –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò PGVECTOR")
            logger.error(f"   {error_msg}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –æ—à–∏–±–∫–∞ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            if "vector" in error_msg.lower() and ("–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç" in error_msg.lower() or "does not exist" in error_msg.lower()):
                logger.error("   –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ PostgreSQL")
                logger.error("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pgvector —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ README/QUICK_START_POSTGRESQL_PGVECTOR.md")
            
            logger.warning("DocumentProcessor –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è")
            import traceback
            logger.debug(f"Traceback: {traceback.format_exc()}")
            logger.info("=" * 60)
            self.vector_repo = None
            self.document_repo = None
            self.vectorstore = None
    
    async def _check_pgvector_extension(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è pgvector –≤ PostgreSQL"""
        try:
            if not self.vector_repo:
                return False
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
            async with self.vector_repo.db_connection.acquire() as conn:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                result = await conn.fetchval(
                    "SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'vector')"
                )
                return bool(result)
        except Exception as e:
            error_msg = str(e)
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–º –æ–ø–µ—Ä–∞—Ü–∏–π, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ
            # (—Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü –≤ repository.py)
            if "another operation is in progress" in error_msg.lower() or "–æ–ø–µ—Ä–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è" in error_msg.lower():
                logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ –∏–∑-–∑–∞ –∞–∫—Ç–∏–≤–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                logger.info("(–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü)")
                return True
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector: {error_msg}")
            # –ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å, –Ω–æ –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ
            # (—Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ repository.py), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True
            logger.info("–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ (—Å–æ–∑–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü)")
            return True
    
    async def _test_pgvector_connection(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ pgvector"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            if not self.vector_repo or not self.document_repo:
                logger.warning("–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                return False
            
            # –ü—Ä–æ–±—É–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ –ë–î
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î
            try:
                documents = await self.document_repo.get_all_documents(limit=1)
                logger.info(f" –í –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)} (–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ 1)")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ë–î: {str(e)}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–∞–±–ª–∏—Ü—ã –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            # –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–æ–ø—ã—Ç–∫—É –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
            try:
                # –ü—Ä–æ–±—É–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ —Å –ø—É—Å—Ç—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º (–ø—Ä–æ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∞–±–ª–∏—Ü—ã)
                test_embedding = [0.0] * 384  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                results = await self.vector_repo.similarity_search(test_embedding, limit=1)
                logger.info(f"–¢–∞–±–ª–∏—Ü–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–∞–π–¥–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {len(results)} (–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ 1)")
            except Exception as e:
                error_msg = str(e)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –æ—à–∏–±–∫–∞ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —Ç–∏–ø–∞ vector
                if "vector" in error_msg.lower() and ("–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç" in error_msg.lower() or "does not exist" in error_msg.lower()):
                    logger.error(f"–¢–∞–±–ª–∏—Ü–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞: {error_msg}")
                    logger.error("   –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞")
                    return False
                else:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –≤–µ–∫—Ç–æ—Ä–æ–≤: {error_msg}")
                    # –≠—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞
                    logger.info("   (–¢–∞–±–ª–∏—Ü–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
            
            return True
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏: {error_msg}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –æ—à–∏–±–∫–∞ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            if "vector" in error_msg.lower() and ("–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç" in error_msg.lower() or "does not exist" in error_msg.lower()):
                logger.error("   –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ PostgreSQL")
            
            import traceback
            logger.debug(f"   Traceback: {traceback.format_exc()}")
            return False
    
    def _load_documents_from_db(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if not self.document_repo:
            logger.warning("DocumentRepository –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return
        
        try:
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ë–î (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ async –º–µ—Ç–æ–¥–∞)
            pg_documents = asyncio.run(self.document_repo.get_all_documents(limit=1000))
            
            loaded_count = 0
            for pg_doc in pg_documents:
                filename = pg_doc.filename
                if filename not in self.doc_names:
                    self.doc_names.append(filename)
                    loaded_count += 1
                self.filename_to_id[filename] = pg_doc.id
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                if pg_doc.metadata:
                    if "confidence_data" in pg_doc.metadata:
                        self.confidence_data[filename] = pg_doc.metadata["confidence_data"]
            
            if loaded_count > 0:
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
                logger.info(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ: {len(self.doc_names)}")
            else:
                logger.info("–î–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –±–∞–∑–∞ –ø—É—Å—Ç–∞")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ë–î: {str(e)}")
            import traceback
            logger.debug(f"Traceback: {traceback.format_exc()}")
    
    async def process_document(self, file_data: bytes, filename: str, file_extension: str, minio_object_name=None, minio_bucket=None):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ–≥–æ —Ç–∏–ø–∞
        
        Args:
            file_data: –î–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ –≤ –≤–∏–¥–µ bytes
            filename: –ò–º—è —Ñ–∞–π–ª–∞ (–¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            file_extension: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '.pdf', '.docx')
            minio_object_name: –ò–º—è –æ–±—ä–µ–∫—Ç–∞ –≤ MinIO (–µ—Å–ª–∏ —Ñ–∞–π–ª —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ MinIO)
            minio_bucket: –ò–º—è bucket –≤ MinIO (–µ—Å–ª–∏ —Ñ–∞–π–ª —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ MinIO)
        """
        file_extension = file_extension.lower()
        document_text = ""
        confidence_info = None
        
        try:
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {filename} (—Ç–∏–ø: {file_extension}, —Ä–∞–∑–º–µ—Ä: {len(file_data)} –±–∞–π—Ç)")
            
            if file_extension == '.docx':
                result = self.extract_text_from_docx_bytes(file_data)
                if isinstance(result, dict):
                    document_text = result.get("text", "")
                    confidence_info = result.get("confidence_info", {"confidence": 100.0, "text_length": len(document_text), "file_type": "docx", "words": []})
                else:
                    document_text = result
                    confidence_info = self._create_confidence_info_for_text(document_text, 100.0, "docx")
            elif file_extension == '.pdf':
                result = self.extract_text_from_pdf_bytes(file_data)
                if isinstance(result, dict):
                    document_text = result.get("text", "")
                    confidence_info = result.get("confidence_info", {"confidence": 100.0, "text_length": len(document_text), "file_type": "pdf", "words": []})
                else:
                    document_text = result
                    confidence_info = self._create_confidence_info_for_text(document_text, 100.0, "pdf")
            elif file_extension in ['.xlsx', '.xls']:
                result = self.extract_text_from_excel_bytes(file_data)
                if isinstance(result, dict):
                    document_text = result.get("text", "")
                    confidence_info = result.get("confidence_info", {"confidence": 100.0, "text_length": len(document_text), "file_type": "excel", "words": []})
                else:
                    document_text = result
                    confidence_info = self._create_confidence_info_for_text(document_text, 100.0, "excel")
            elif file_extension == '.txt':
                result = self.extract_text_from_txt_bytes(file_data)
                if isinstance(result, dict):
                    document_text = result.get("text", "")
                    confidence_info = result.get("confidence_info", {"confidence": 100.0, "text_length": len(document_text), "file_type": "txt", "words": []})
                else:
                    document_text = result
                    confidence_info = self._create_confidence_info_for_text(document_text, 100.0, "txt")
            elif file_extension in ['.jpg', '.jpeg', '.png', '.webp']:
                result = self.extract_text_from_image_bytes(file_data)
                if isinstance(result, dict):
                    document_text = result.get("text", "")
                    confidence_info = result.get("confidence_info", {"confidence": 0.0, "text_length": len(document_text), "file_type": "image", "words": []})
                    # –ï—Å–ª–∏ OCR –Ω–µ —É–¥–∞–ª—Å—è (–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∞), –ª–æ–≥–∏—Ä—É–µ–º, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
                    if not document_text and confidence_info.get("error"):
                        error_msg = confidence_info.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
                        print(f"–í–ù–ò–ú–ê–ù–ò–ï: OCR –Ω–µ —É–¥–∞–ª—Å—è –¥–ª—è {filename}: {error_msg}")
                        print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ, –Ω–æ —Ç–µ–∫—Å—Ç –Ω–µ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω")
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É - –¥–æ–∫—É–º–µ–Ω—Ç –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º
                        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Å–µ —Ä–∞–≤–Ω–æ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
                else:
                    document_text = result
                    confidence_info = self._create_confidence_info_for_text(document_text, 50.0, "image")
            else:
                return False, f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_extension}"
            
            print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–µ–∫—Å—Ç–∞: {len(document_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if confidence_info:
                self.confidence_data[filename] = confidence_info
                print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è {filename}: {confidence_info['confidence']:.2f}%")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –≤ MinIO, –µ—Å–ª–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if file_extension in ['.jpg', '.jpeg', '.png', '.webp']:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ MinIO –æ–±—ä–µ–∫—Ç–µ
                if minio_object_name and minio_bucket:
                    self.image_paths[filename] = {
                        "minio_object": minio_object_name,
                        "minio_bucket": minio_bucket,
                        "file_data": file_data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                    }
                    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –≤ MinIO –¥–ª—è {filename}: {minio_bucket}/{minio_object_name}")
                else:
                    # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏
                    self.image_paths[filename] = {
                        "file_data": file_data
                    }
                    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è {filename}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, OCR –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª), –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            # —á—Ç–æ–±—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±—ã–ª–æ –¥–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            if document_text or file_extension in ['.jpg', '.jpeg', '.png', '.webp']:
                # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–±–∞–≤–ª—è–µ–º –¥–∞–∂–µ —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º async –≤–µ—Ä—Å–∏—é, —Ç–∞–∫ –∫–∞–∫ process_document —Ç–µ–ø–µ—Ä—å async
                await self.add_document_to_collection_async(document_text, filename)
                print(f"–î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é. –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.doc_names)}")
            else:
                print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {filename} - —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏ —ç—Ç–æ –Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            
            return True, f"–î–æ–∫—É–º–µ–Ω—Ç {filename} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω"
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
            return False, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}"
    
    def _create_confidence_info_for_text(self, text, confidence_per_word, file_type):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        import re
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞: —Ä–∞–∑–¥–µ–ª—è–µ–º —Å–ª–æ–≤–∞ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞ (–±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –¥–µ—Ñ–∏—Å—ã –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤) –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
        # –ü–∞—Ç—Ç–µ—Ä–Ω: \w+ –¥–ª—è —Å–ª–æ–≤ (–≤–∫–ª—é—á–∞—è –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è), –∏–ª–∏ [^\w\s] –¥–ª—è –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        # –ù–æ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞—Ö–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
        tokens = re.findall(r'\w+|[^\w\s]+', text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤
        words_with_confidence = []
        for token in tokens:
            token = token.strip()
            if token:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã
                words_with_confidence.append({"word": token, "confidence": float(confidence_per_word)})
        
        avg_confidence = confidence_per_word
        
        return {
            "confidence": avg_confidence,
            "text_length": len(text),
            "file_type": file_type,
            "words": words_with_confidence
        }
    
    def extract_text_from_docx_bytes(self, file_data: bytes):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞ –∏–∑ bytes"""
        print(f"–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ DOCX —Ñ–∞–π–ª–∞ (—Ä–∞–∑–º–µ—Ä: {len(file_data)} –±–∞–π—Ç)")
        doc = docx.Document(BytesIO(file_data))
        full_text = []
        
        for para in doc.paragraphs:
            full_text.append(para.text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    full_text.append(cell.text)
        
        result = "\n".join(full_text)
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ DOCX")
        return result
    
    def extract_text_from_pdf_bytes(self, file_data: bytes):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞ –∏–∑ bytes —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        print(f"–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ (—Ä–∞–∑–º–µ—Ä: {len(file_data)} –±–∞–π—Ç)")
        text = ""
        confidence_scores = []
        total_chars = 0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º PDFPlumber –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        try:
            with pdfplumber.open(BytesIO(file_data)) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text() or ""
                    text += page_text
                    total_chars += len(page_text)
                    
                    # –î–ª—è PDF –æ—Ü–µ–Ω–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è —É—Å–ø–µ—à–Ω–æ, —Å—á–∏—Ç–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–æ–π
                    if page_text.strip():
                        # PDFPlumber –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è
                        confidence_scores.append(95.0)  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–ø–µ—à–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    else:
                        confidence_scores.append(50.0)  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
                        
            print(f"PDFPlumber —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é pdfplumber: {str(e)}")
            
            # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Å PyPDF2
            try:
                reader = PyPDF2.PdfReader(BytesIO(file_data))
                for page in reader.pages:
                    page_text = page.extract_text() or ""
                    text += page_text
                    total_chars += len(page_text)
                    if page_text.strip():
                        confidence_scores.append(85.0)  # –ù–µ–º–Ω–æ–≥–æ –Ω–∏–∂–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è PyPDF2
                    else:
                        confidence_scores.append(40.0)
                print(f"PyPDF2 —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            except Exception as e2:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é PyPDF2: {str(e2)}")
                raise
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω (—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π PDF), –ø—Ä–æ–±—É–µ–º OCR —á–µ—Ä–µ–∑ Surya
        if not text or len(text.strip()) == 0:
            print("PDF –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç. –ü—Ä–æ–±—É–µ–º OCR —á–µ—Ä–µ–∑ Surya...")
            try:
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pdf2image –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                try:
                    from pdf2image import convert_from_bytes
                    from backend.llm_client import recognize_text_from_image_llm_svc
                    from PIL import Image
                    import io
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ bytes
                    images = convert_from_bytes(file_data, dpi=300)
                    print(f"PDF –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è OCR")
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º OCR –∫ –∫–∞–∂–¥–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —á–µ—Ä–µ–∑ Surya
                    ocr_text = ""
                    ocr_confidence_scores = []
                    for i, image in enumerate(images):
                        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {i+1}/{len(images)} —Å –ø–æ–º–æ—â—å—é Surya OCR...")
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL Image –≤ bytes
                        img_bytes = io.BytesIO()
                        image.save(img_bytes, format='PNG')
                        img_bytes.seek(0)
                        page_image_data = img_bytes.getvalue()
                        
                        # –í—ã–∑—ã–≤–∞–µ–º OCR —á–µ—Ä–µ–∑ llm-svc API
                        result = recognize_text_from_image_llm_svc(
                            image_file=page_image_data,
                            filename=f"page_{i+1}.png",
                            languages="ru,en"
                        )
                        
                        if result.get("success", False):
                            page_text = result.get("text", "")
                            page_confidence = result.get("confidence", 50.0)
                            ocr_text += f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1} ---\n{page_text}\n"
                            ocr_confidence_scores.append(page_confidence)
                        else:
                            print(f"–û—à–∏–±–∫–∞ OCR –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i+1}: {result.get('error', 'Unknown error')}")
                            ocr_confidence_scores.append(50.0)
                    
                    if ocr_text.strip():
                        text = ocr_text
                        confidence_scores = ocr_confidence_scores
                        print(f"Surya OCR —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(images)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                    else:
                        print("Surya OCR –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF")
                except ImportError:
                    print("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
                    print("–¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å poppler: https://github.com/oschwartz10612/poppler-windows/releases")
                except Exception as ocr_error:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ OCR –æ–±—Ä–∞–±–æ—Ç–∫–µ PDF —á–µ—Ä–µ–∑ Surya: {ocr_error}")
                    import traceback
                    traceback.print_exc()
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å OCR –∫ PDF: {e}")
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (–¥–ª—è PDF –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ 100%)
        confidence_per_word = 100.0 if avg_confidence > 90.0 else avg_confidence
        confidence_info = self._create_confidence_info_for_text(text, confidence_per_word, "pdf")
        confidence_info["pages_processed"] = len(confidence_scores)
        
        return {
            "text": text,
            "confidence_info": confidence_info
        }
    
    def extract_text_from_excel(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Excel —Ñ–∞–π–ª–∞"""
        print(f"–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ Excel —Ñ–∞–π–ª–∞: {file_path}")
        workbook = openpyxl.load_workbook(file_path, data_only=True)
        text_content = []
        
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]
            text_content.append(f"–õ–∏—Å—Ç: {sheet_name}")
            
            for row in sheet.iter_rows():
                row_values = []
                for cell in row:
                    if cell.value is not None:
                        row_values.append(str(cell.value))
                if row_values:
                    text_content.append("\t".join(row_values))
        
        result = "\n".join(text_content)
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ Excel")
        return result
    
    def extract_text_from_txt_bytes(self, file_data: bytes):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ TXT —Ñ–∞–π–ª–∞ –∏–∑ bytes"""
        print(f"–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ TXT —Ñ–∞–π–ª–∞ (—Ä–∞–∑–º–µ—Ä: {len(file_data)} –±–∞–π—Ç)")
        try:
            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ UTF-8
            result = file_data.decode('utf-8')
            print(f"UTF-8 —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            return result
        except UnicodeDecodeError:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ UTF-8, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            encodings = ['cp1251', 'latin-1', 'koi8-r']
            for encoding in encodings:
                try:
                    result = file_data.decode(encoding)
                    print(f"{encoding} —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return result
                except UnicodeDecodeError:
                    continue
            
            # –ï—Å–ª–∏ –≤—Å–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –Ω–µ –ø–æ–¥–æ—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
            result = str(file_data)
            print(f"–ë–∏–Ω–∞—Ä–Ω—ã–π —Ä–µ–∂–∏–º –∏–∑–≤–ª–µ–∫ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            return result

    def extract_text_from_image_bytes(self, file_data: bytes):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ bytes —Å –ø–æ–º–æ—â—å—é Surya OCR —á–µ—Ä–µ–∑ llm-svc API —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        print(f"–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Surya OCR (—Ä–∞–∑–º–µ—Ä: {len(file_data)} –±–∞–π—Ç)")
        print(f"DEBUG: –ù–∞—á–∏–Ω–∞–µ–º –≤—ã–∑–æ–≤ OCR —á–µ—Ä–µ–∑ llm-svc API...")
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OCR —á–µ—Ä–µ–∑ llm-svc
            from backend.llm_client import recognize_text_from_image_llm_svc
            from PIL import Image
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img = Image.open(BytesIO(file_data))
            filename = "image.jpg"
            if img.format:
                filename = f"image.{img.format.lower()}"
            
            print(f"DEBUG: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–æ, —Ñ–æ—Ä–º–∞—Ç: {img.format}, —Ä–∞–∑–º–µ—Ä: {img.size}")
            
            # –í—ã–∑—ã–≤–∞–µ–º OCR —á–µ—Ä–µ–∑ llm-svc API
            print("–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ llm-svc...")
            print(f"DEBUG: –í—ã–∑—ã–≤–∞–µ–º recognize_text_from_image_llm_svc —Å filename={filename}, languages=ru,en")
            try:
                result = recognize_text_from_image_llm_svc(
                    image_file=file_data,
                    filename=filename,
                    languages="ru,en"
                )
                print(f"DEBUG: OCR –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç: success={result.get('success', False)}")
            except Exception as ocr_exception:
                print(f"DEBUG: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OCR: {ocr_exception}")
                import traceback
                traceback.print_exc()
                raise
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not result.get("success", False):
                error_msg = result.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
                print(f"Surya OCR –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {error_msg}")
                print(f"–í–ù–ò–ú–ê–ù–ò–ï: OCR –Ω–µ —É–¥–∞–ª—Å—è, —Ç–µ–∫—Å—Ç –Ω–µ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
                # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
                return {
                    "text": "",  # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ
                    "confidence_info": {
                        "confidence": 0.0,
                        "text_length": 0,
                        "file_type": "image",
                        "ocr_available": False,
                        "error": error_msg,
                        "words": []
                    }
                }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            text = result.get("text", "")
            words_with_confidence = result.get("words", [])
            avg_confidence = result.get("confidence", 0.0)
            word_count = result.get("words_count", 0)
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
            if not text.strip():
                print(f"Surya OCR –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π)")
                # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
                return {
                    "text": "",  # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
                    "confidence_info": {
                        "confidence": 0.0,
                        "text_length": 0,
                        "file_type": "image",
                        "ocr_available": False,
                        "words": []
                    }
                }
            
            print(f"Surya OCR —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ–∫ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, {word_count} —Å–ª–æ–≤, —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.2f}%")
            
            return {
                "text": text,
                "confidence_info": {
                    "confidence": avg_confidence,
                    "text_length": len(text),
                    "file_type": "image",
                    "ocr_available": True,
                    "words": words_with_confidence
                }
            }
        except ImportError:
            # –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            result_text = f"[–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –î–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø –∫ llm-svc API.]"
            print(f"–§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ llm-svc –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ: {len(result_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return {
                "text": result_text,
                "confidence_info": {
                    "confidence": 0.0,
                    "text_length": len(result_text),
                    "file_type": "image",
                    "ocr_available": False,
                    "words": []
                }
            }
        except Exception as e:
            error_msg = str(e)
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Surya OCR: {error_msg}")
            import traceback
            traceback.print_exc()
            
            # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –∫–∞–∫ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
            result_text = ""  # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ
            print(f"–í–ù–ò–ú–ê–ù–ò–ï: OCR –Ω–µ —É–¥–∞–ª—Å—è, —Ç–µ–∫—Å—Ç –Ω–µ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return {
                "text": result_text,
                "confidence_info": {
                    "confidence": 0.0,
                    "text_length": 0,
                    "file_type": "image",
                    "ocr_available": False,
                    "error": error_msg,
                    "words": []
                }
            }
    
    async def add_document_to_collection_async(self, text, doc_name):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc_name}' –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é...")
        logger.info(f"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ = –º–µ–Ω—å—à–µ —á–∞–Ω–∫–æ–≤ = –±—ã—Å—Ç—Ä–µ–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞, –Ω–æ –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∫–∞–∂–¥–æ–º —á–∞–Ω–∫–µ
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤ –∏ –±–æ–ª—å—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
            chunk_overlap=200,  # –£–º–µ—Ä–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
            length_function=len,
        )
        
        chunks = text_splitter.split_text(text)
        print(f"–°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —á–∞–Ω–∫–æ–≤ (–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç), —Å–æ–∑–¥–∞–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —á–∞–Ω–∫
        if len(chunks) == 0:
            print(f"–í–ù–ò–ú–ê–ù–ò–ï: –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc_name}', —Å–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —á–∞–Ω–∫")
            chunks = [text] if text else [f"[–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}]"]
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è langchain (–¥–ª—è –∫—ç—à–∞)
        langchain_docs = []
        for i, chunk in enumerate(chunks):
            langchain_docs.append(
                Document(
                    page_content=chunk,
                    metadata={"source": doc_name, "chunk": i}
                )
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∫—ç—à)
        self.documents.extend(langchain_docs)
        if doc_name not in self.doc_names:
            self.doc_names.append(doc_name)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self._doc_chunks_cache[doc_name] = [
            {
                "content": doc.page_content,
                "chunk": doc.metadata.get("chunk", i)
            }
            for i, doc in enumerate(langchain_docs)
        ]
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —á–∞–Ω–∫–∞
        self._doc_chunks_cache[doc_name].sort(key=lambda x: x['chunk'])
        
        print(f"–î–æ–∫—É–º–µ–Ω—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫—ç—à. –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}, –∏–º–µ–Ω: {len(self.doc_names)}")
        print(f"–ö—ç—à —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è '{doc_name}': {len(self._doc_chunks_cache[doc_name])} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ PostgreSQL + pgvector
        if self.vector_repo and self.document_repo:
            try:
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc_name}' –≤ PostgreSQL + pgvector...")
                await self._save_document_to_pgvector(text, doc_name, chunks)
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç '{doc_name}' —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ PostgreSQL + pgvector")
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ vectorstore –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                self.vectorstore = True
            except Exception as e:
                logger.error(f"–û–®–ò–ë–ö–ê –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ PostgreSQL: {str(e)}")
                import traceback
                logger.debug(f"Traceback: {traceback.format_exc()}")
                # –î–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç–∏, –Ω–æ vectorstore –æ—Å—Ç–∞–µ—Ç—Å—è None
        else:
            logger.warning(f"PostgreSQL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –¥–æ–∫—É–º–µ–Ω—Ç '{doc_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ç–æ–ª—å–∫–æ –≤ –ø–∞–º—è—Ç–∏")
            # –ï—Å–ª–∏ pgvector –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã –µ—Å—Ç—å –≤ –ø–∞–º—è—Ç–∏, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö
            if self.doc_names:
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –ø–∞–º—è—Ç–∏: {len(self.doc_names)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def add_document_to_collection(self, text, doc_name):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π event loop
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # –ï—Å–ª–∏ event loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
                # –ù–æ —ç—Ç–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º run_until_complete —Å –Ω–æ–≤—ã–º loop
                # –ò–ª–∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ —Ñ–æ–Ω–µ
                import concurrent.futures
                import threading
                
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è async –æ–ø–µ—Ä–∞—Ü–∏–∏
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        new_loop.run_until_complete(self.add_document_to_collection_async(text, doc_name))
                    finally:
                        new_loop.close()
                
                thread = threading.Thread(target=run_in_thread)
                thread.start()
                thread.join()  # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            else:
                # –ï—Å–ª–∏ loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å run_until_complete
                loop.run_until_complete(self.add_document_to_collection_async(text, doc_name))
        except RuntimeError:
            # –ï—Å–ª–∏ –Ω–µ—Ç event loop, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            asyncio.run(self.add_document_to_collection_async(text, doc_name))
    
    async def _save_document_to_pgvector(self, text: str, doc_name: str, chunks: List[str]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ PostgreSQL —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if not self.embeddings:
            logger.error("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç
            document_id = self.filename_to_id.get(doc_name)
            
            # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ë–î
            if document_id is None:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                logger.info(f"   –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î: '{doc_name}'")
                pg_doc = PGDocument(
                    filename=doc_name,
                    content=text,
                    metadata={
                        "confidence_data": self.confidence_data.get(doc_name, {}),
                        "chunks_count": len(chunks)
                    },
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                document_id = await self.document_repo.create_document(pg_doc)
                if document_id:
                    self.filename_to_id[doc_name] = document_id
                    logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –≤ –ë–î: ID={document_id}")
                else:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ë–î")
                    return
            else:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î: ID={document_id}")
                pg_doc = await self.document_repo.get_document(document_id)
                if pg_doc:
                    pg_doc.content = text
                    pg_doc.metadata = {
                        "confidence_data": self.confidence_data.get(doc_name, {}),
                        "chunks_count": len(chunks)
                    }
                    pg_doc.updated_at = datetime.utcnow()
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
                    await self.vector_repo.delete_vectors_by_document(document_id)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã
            logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤...")
            saved_vectors = 0
            for i, chunk in enumerate(chunks):
                try:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                    embedding = self.embeddings.embed_query(chunk)
                    
                    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä
                    vector = DocumentVector(
                        document_id=document_id,
                        chunk_index=i,
                        embedding=embedding,
                        content=chunk,
                        metadata={"source": doc_name, "chunk": i}
                    )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä –≤ –ë–î
                    vector_id = await self.vector_repo.create_vector(vector)
                    if vector_id:
                        saved_vectors += 1
                        if (i + 1) % 10 == 0 or i == len(chunks) - 1:
                            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {i+1}/{len(chunks)}")
                except Exception as e:
                    logger.warning(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–∞ {i}: {str(e)}")
                    continue
            
            if saved_vectors == len(chunks):
                logger.info(f"–í—Å–µ {saved_vectors} –≤–µ–∫—Ç–æ—Ä–æ–≤ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ pgvector")
            else:
                logger.warning(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_vectors}/{len(chunks)} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
        except Exception as e:
            logger.error(f"–û–®–ò–ë–ö–ê –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ pgvector: {str(e)}")
            import traceback
            logger.debug(f"Traceback: {traceback.format_exc()}")
            raise
    
    def update_vectorstore(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        # –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ pgvector –≤–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Å—Ä–∞–∑—É –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if self.vector_repo:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è pgvector - –≤–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            self.vectorstore = True
        else:
            print("–í–ù–ò–ú–ê–ù–ò–ï: pgvector –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ")
            self.vectorstore = None
    
    async def query_documents_async(self, query, k=2):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        logger.info(f"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ vector_repo (pgvector) –≤–º–µ—Å—Ç–æ vectorstore
        if not self.vector_repo:
            logger.warning("pgvector –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (vector_repo is None)")
            return "–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∏–ª–∏ pgvector –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if not self.doc_names or len(self.doc_names) == 0:
            logger.warning("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return "–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        
        if not self.embeddings:
            logger.error("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return "–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞"
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:100]}...'")
            logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞: k={k}, doc_names={self.doc_names}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pgvector –¥–ª—è –ø–æ–∏—Å–∫–∞
            results = await self._query_documents_async(query, k)
            if isinstance(results, list):
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ pgvector")
                if len(results) == 0:
                    logger.warning("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ pgvector.")
            elif isinstance(results, str):
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ (—Å—Ç—Ä–æ–∫–∞): {results}")
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {str(e)}"
    
    def query_documents(self, query, k=2):
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        try:
            loop = asyncio.get_running_loop()
            # –ï—Å–ª–∏ loop –∑–∞–ø—É—â–µ–Ω, —ç—Ç–æ –æ—à–∏–±–∫–∞ - –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å async –≤–µ—Ä—Å–∏—é
            logger.error("query_documents –≤—ã–∑–≤–∞–Ω –∏–∑ async –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ query_documents_async()")
            return "–û—à–∏–±–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ async –≤–µ—Ä—Å–∏—é –º–µ—Ç–æ–¥–∞"
        except RuntimeError:
            # –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ loop, –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.run
            return asyncio.run(self.query_documents_async(query, k))
    
    async def _query_documents_async(self, query: str, k: int = 2):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ pgvector"""
        logger.debug(f"   –í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ pgvector —Å k={k}...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embeddings.embed_query(query)
        logger.debug(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(query_embedding)})")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ pgvector
        results = await self.vector_repo.similarity_search(query_embedding, limit=k)
        logger.debug(f"   pgvector –≤–µ—Ä–Ω—É–ª {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        formatted_results = []
        for vector, similarity in results:
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –ø–æ document_id
            doc_name = None
            for filename, doc_id in self.filename_to_id.items():
                if doc_id == vector.document_id:
                    doc_name = filename
                    break
            
            if doc_name is None:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –ë–î
                pg_doc = await self.document_repo.get_document(vector.document_id)
                if pg_doc:
                    doc_name = pg_doc.filename
                    self.filename_to_id[doc_name] = vector.document_id
            
            result = {
                "content": vector.content,
                "source": doc_name or f"document_{vector.document_id}",
                "chunk": vector.chunk_index,
                "similarity": similarity
            }
            formatted_results.append(result)
            logger.debug(f"{result['source']}, —á–∞–Ω–∫ {result['chunk']}, similarity: {similarity:.4f}")
        
        return formatted_results
    
    def get_document_list(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print(f"get_document_list –≤—ã–∑–≤–∞–Ω. –î–æ–∫—É–º–µ–Ω—Ç—ã: {self.doc_names}")
        return self.doc_names
    
    def get_image_paths(self):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º (–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã, —Å–∫–∞—á–∞–Ω–Ω—ã–µ –∏–∑ MinIO –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        """
        image_paths_list = []
        print(f"DEBUG get_image_paths: image_paths = {self.image_paths}")
        
        for filename, path_info in self.image_paths.items():
            print(f"DEBUG get_image_paths: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {filename}, path_info = {path_info}")
            
            if isinstance(path_info, dict):
                # –ï—Å–ª–∏ –µ—Å—Ç—å file_data, —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if "file_data" in path_info:
                    try:
                        import tempfile
                        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                        suffix = os.path.splitext(filename)[1] or ".jpg"
                        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
                        temp_file.write(path_info["file_data"])
                        temp_file.close()
                        temp_path = temp_file.name
                        print(f"DEBUG get_image_paths: —Å–æ–∑–¥–∞–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è {filename}: {temp_path}")
                        image_paths_list.append(temp_path)
                    except Exception as e:
                        print(f"ERROR get_image_paths: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è {filename}: {e}")
                        image_paths_list.append(None)
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                elif "path" in path_info:
                    image_paths_list.append(path_info.get("path"))
                else:
                    print(f"WARNING get_image_paths: –Ω–µ—Ç file_data –∏–ª–∏ path –¥–ª—è {filename}")
                    image_paths_list.append(None)
            else:
                # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –ø—Ä–æ—Å—Ç–æ –ø—É—Ç—å
                image_paths_list.append(path_info)
        
        print(f"get_image_paths –≤—ã–∑–≤–∞–Ω. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_paths_list}")
        return image_paths_list
    
    def get_image_minio_info(self, filename):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ MinIO –æ–±—ä–µ–∫—Ç–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Returns:
            dict: {"minio_object": str, "minio_bucket": str} –∏–ª–∏ None
        """
        if filename in self.image_paths:
            path_info = self.image_paths[filename]
            if isinstance(path_info, dict):
                return {
                    "minio_object": path_info.get("minio_object"),
                    "minio_bucket": path_info.get("minio_bucket")
                }
        return None
    
    def clear_documents(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print("–û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        self.documents = []
        self.doc_names = []
        self.vectorstore = None
        self.confidence_data = {}
        self.image_paths = {}
        self._doc_chunks_cache = {}  # –û—á–∏—â–∞–µ–º –∫—ç—à
        self.filename_to_id = {}  # –û—á–∏—â–∞–µ–º –º–∞–ø–ø–∏–Ω–≥
        
        # –û—á–∏—â–∞–µ–º –∏–∑ PostgreSQL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –ë–î)
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –ë–î, —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ:
        # if self.document_repo:
        #     # –£–¥–∞–ª—è–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ë–î
        #     asyncio.run(self._clear_all_documents_from_db())
        
        print("–ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—á–∏—â–µ–Ω–∞")
        return "–ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—á–∏—â–µ–Ω–∞"
    
    def get_confidence_report_data(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–∞ –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏ –Ω–∞–¥ —Å–ª–æ–≤–∞–º–∏"""
        if not self.confidence_data:
            return {
                "total_documents": 0,
                "documents": [],
                "average_confidence": 0.0,
                "formatted_texts": []
            }
        
        documents = []
        formatted_texts = []
        total_confidence = 0.0
        total_weighted_confidence = 0.0
        total_words = 0
        
        for filename, info in self.confidence_data.items():
            words = info.get("words", [])
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏ –Ω–∞–¥ —Å–ª–æ–≤–∞–º–∏
            formatted_lines = []
            current_line = []
            
            for word_info in words:
                word = word_info.get("word", "")
                conf = word_info.get("confidence", 0.0)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–ª–æ–≤–∞
                if not word:
                    continue
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ —Å –ø—Ä–æ—Ü–µ–Ω—Ç–æ–º –Ω–∞–¥ –Ω–∏–º
                formatted_word = f"{conf:.0f}%\n{word}"
                current_line.append(formatted_word)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                if len(current_line) >= 10:  # –ü—Ä–∏–º–µ—Ä–Ω–æ 10 —Å–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–æ–∫—É
                    formatted_lines.append("  ".join(current_line))
                    current_line = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞
            if current_line:
                formatted_lines.append("  ".join(current_line))
            
            formatted_text = "\n".join(formatted_lines)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_avg_confidence = info.get("confidence", 0.0)
            if words:
                doc_avg_confidence = sum(w.get("confidence", 0.0) for w in words) / len(words)
            
            documents.append({
                "filename": filename,
                "confidence": doc_avg_confidence,
                "text_length": info.get("text_length", 0),
                "file_type": info.get("file_type", "unknown"),
                "words_count": len(words)
            })
            
            formatted_texts.append({
                "filename": filename,
                "formatted_text": formatted_text,
                "words": words
            })
            
            total_confidence += doc_avg_confidence
            if words:
                total_weighted_confidence += sum(w.get("confidence", 0.0) for w in words)
                total_words += len(words)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        avg_confidence = total_confidence / len(documents) if documents else 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º —Å–ª–æ–≤–∞–º
        overall_confidence = total_weighted_confidence / total_words if total_words > 0 else avg_confidence
        
        return {
            "total_documents": len(documents),
            "documents": documents,
            "average_confidence": avg_confidence,
            "overall_confidence": overall_confidence,
            "total_words": total_words,
            "formatted_texts": formatted_texts
        }
    
    def process_query(self, query, agent_function):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è LLM"""
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å: {query}")
        print(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {self.vectorstore is not None}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        print(f"–ò–º–µ–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.doc_names}")
        
        if not self.vectorstore:
            return "–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞."
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            docs = self.query_documents(query)
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(docs) if isinstance(docs, list) else '–æ—à–∏–±–∫–∞'}")
            
            if isinstance(docs, str):  # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {docs}")
                return docs
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            context = "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n\n"
            for i, doc in enumerate(docs):
                context += f"–§—Ä–∞–≥–º–µ–Ω—Ç {i+1} (–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc['source']}'):\n{doc['content']}\n\n"
            
            print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω, –¥–ª–∏–Ω–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è LLM —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —É–∫–∞–∂–∏ —ç—Ç–æ.
–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–û—Ç–≤–µ—Ç:"""
            
            print("–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM...")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            response = agent_function(prompt)
            print(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç LLM, –¥–ª–∏–Ω–∞: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤")
            return response
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"
    
    def remove_document(self, filename):
        """–£–¥–∞–ª–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        print(f"–£–¥–∞–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {filename}")
        print(f"–î–æ —É–¥–∞–ª–µ–Ω–∏—è - self.doc_names: {self.doc_names}")
        print(f"–î–æ —É–¥–∞–ª–µ–Ω–∏—è - self.documents: {len(self.documents)}")
        print(f"–î–æ —É–¥–∞–ª–µ–Ω–∏—è - self.vectorstore –¥–æ—Å—Ç—É–ø–µ–Ω: {self.vectorstore is not None}")
        
        try:
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if filename not in self.doc_names:
                print(f"–î–æ–∫—É–º–µ–Ω—Ç {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return False
            
            # –£–¥–∞–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–º–µ–Ω
            index = self.doc_names.index(filename)
            self.doc_names.pop(index)
            print(f"–î–æ–∫—É–º–µ–Ω—Ç {filename} —É–¥–∞–ª–µ–Ω –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–º–µ–Ω")
            
            # –£–¥–∞–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if filename in self.confidence_data:
                del self.confidence_data[filename]
                print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è {filename} —É–¥–∞–ª–µ–Ω–∞")
            
            # –£–¥–∞–ª—è–µ–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –µ—Å–ª–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if filename in self.image_paths:
                del self.image_paths[filename]
                print(f"–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–ª—è {filename} —É–¥–∞–ª–µ–Ω")
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∫—ç—à–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if filename in self._doc_chunks_cache:
                del self._doc_chunks_cache[filename]
                print(f"–ö—ç—à —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è {filename} —É–¥–∞–ª–µ–Ω")
            
            # –£–¥–∞–ª—è–µ–º –í–°–ï —á–∞–Ω–∫–∏ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            # –ò—â–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —ç—Ç–∏–º –∏–º–µ–Ω–µ–º –∏ —É–¥–∞–ª—è–µ–º –∏—Ö
            documents_to_remove = []
            for i, doc in enumerate(self.documents):
                if doc.metadata.get("source") == filename:
                    documents_to_remove.append(i)
            
            # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ–±—ã –∏–Ω–¥–µ–∫—Å—ã –Ω–µ —Å–¥–≤–∏–≥–∞–ª–∏—Å—å
            for i in reversed(documents_to_remove):
                self.documents.pop(i)
            
            print(f"–£–¥–∞–ª–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {filename}: {len(documents_to_remove)}")
            print(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è - self.doc_names: {self.doc_names}")
            print(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è - self.documents: {len(self.documents)}")
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ PostgreSQL + pgvector
            if self.document_repo and filename in self.filename_to_id:
                try:
                    document_id = self.filename_to_id[filename]
                    # –£–¥–∞–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
                    try:
                        loop = asyncio.get_running_loop()
                        # –ï—Å–ª–∏ loop –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º create_task –∏–ª–∏ run –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                        import threading
                        
                        def run_async():
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                new_loop.run_until_complete(self.vector_repo.delete_vectors_by_document(document_id))
                                new_loop.run_until_complete(self.document_repo.delete_document(document_id))
                            finally:
                                new_loop.close()
                        
                        thread = threading.Thread(target=run_async)
                        thread.start()
                        thread.join()
                    except RuntimeError:
                        # –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ loop
                        asyncio.run(self.vector_repo.delete_vectors_by_document(document_id))
                        asyncio.run(self.document_repo.delete_document(document_id))
                    
                    # –£–¥–∞–ª—è–µ–º –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞
                    del self.filename_to_id[filename]
                    print(f"–î–æ–∫—É–º–µ–Ω—Ç {filename} —É–¥–∞–ª–µ–Ω –∏–∑ PostgreSQL + pgvector")
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ PostgreSQL: {str(e)}")
                    import traceback
                    traceback.print_exc()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–ª–∞–≥ vectorstore
            if not self.doc_names:
                print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ—á–∏—â–∞–µ–º vectorstore")
                self.vectorstore = None
            else:
                self.vectorstore = True if self.vector_repo else None
            
            print(f"–î–æ–∫—É–º–µ–Ω—Ç {filename} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω. –û—Å—Ç–∞–ª–æ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.doc_names)}")
            return True
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {filename}: {str(e)}")
            import traceback
            traceback.print_exc()
            return False 
    
    async def get_document_context_async(self, query, k=2, include_all_chunks=None, max_context_length=30000):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            include_all_chunks: –ï—Å–ª–∏ None - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞
                              –ï—Å–ª–∏ True - –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                              If False - —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            max_context_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30000)
        """
        print(f"–ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        logger.info(f"get_document_context –≤—ã–∑–≤–∞–Ω –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:100]}...'")
        logger.info(f"vector_repo –¥–æ—Å—Ç—É–ø–µ–Ω: {self.vector_repo is not None}")
        logger.info(f"doc_names: {self.doc_names}")
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.doc_names) if self.doc_names else 0}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ vector_repo –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if not self.vector_repo:
            logger.warning("pgvector –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (vector_repo is None)")
            print("pgvector –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (vector_repo is None)")
            return None
        
        if not self.doc_names or len(self.doc_names) == 0:
            logger.warning("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            print("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return None
        
        try:
            import time
            start_time = time.time()
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞
            if include_all_chunks is None:
                query_lower = query.lower()
                # –ó–∞–ø—Ä–æ—Å—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                full_context_keywords = [
                    '—Å–∞–º–º–∞—Ä–∏', 'summary', '–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '–æ–±–∑–æ—Ä', '—Ä–µ–∑—é–º–µ',
                    '–ø–æ –≤—Å–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É', '–≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç', '–≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞',
                    '–ø–µ—Ä–µ—Å–∫–∞–∂–∏', '–æ–ø–∏—à–∏ –≤–µ—Å—å', '—Ä–∞—Å—Å–∫–∞–∂–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ', '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞'
                ]
                # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                is_full_context_request = any(keyword in query_lower for keyword in full_context_keywords)
                include_all_chunks = is_full_context_request
                print(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Ä–µ–∂–∏–º: {'–ü–û–õ–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢' if include_all_chunks else '–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´'}")
            else:
                print(f"–†–µ–∂–∏–º: {'–ü–û–õ–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢' if include_all_chunks else '–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´'}")
            
            if include_all_chunks:
                # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –†–ï–ñ–ò–ú: –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤,
                # –∑–∞—Ç–µ–º –±—ã—Å—Ç—Ä–æ –ø–æ–ª—É—á–∞–µ–º –í–°–ï —á–∞–Ω–∫–∏ —ç—Ç–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –ë–î
                
                # –ë—ã—Å—Ç—Ä—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
                docs = await self.query_documents_async(query, k=min(k, 5))  # –ù—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                print(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs) if isinstance(docs, list) else '–æ—à–∏–±–∫–∞'}")
                
                if isinstance(docs, str):  # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {docs}")
                    return None
                
                # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                doc_names_found = set()
                for doc in docs:
                    if isinstance(doc, dict) and 'source' in doc:
                        doc_names_found.add(doc['source'])
                
                # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                if not doc_names_found and self.doc_names:
                    doc_names_found = set(self.doc_names)
                    print(f"–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ: {list(doc_names_found)}")
                
                # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø: –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –∫–ª—é—á–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
                # –≤–º–µ—Å—Ç–æ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                all_chunks = []
                for doc_name in doc_names_found:
                    if doc_name in self._doc_chunks_cache:
                        cached_chunks = self._doc_chunks_cache[doc_name]
                        total_chunks = len(cached_chunks)
                        
                        # –í—ã–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —á–∞–Ω–∫–∏: –Ω–∞—á–∞–ª–æ, –∫–æ–Ω–µ—Ü, –∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ
                        selected_chunks = []
                        
                        # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫
                        if cached_chunks:
                            selected_chunks.append(cached_chunks[0])
                        
                        # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
                        if len(cached_chunks) > 1:
                            selected_chunks.append(cached_chunks[-1])
                        
                        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 15-20 –¥–ª—è –±–∞–ª–∞–Ω—Å–∞)
                        target_chunks = min(18, total_chunks)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                        if total_chunks > 2:
                            step = max(1, total_chunks // target_chunks)
                            for i in range(step, total_chunks - 1, step):
                                if cached_chunks[i] not in selected_chunks:
                                    selected_chunks.append(cached_chunks[i])
                        
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —á–∞–Ω–∫–∞
                        selected_chunks.sort(key=lambda x: x['chunk'])
                        
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                        for chunk_data in selected_chunks:
                            all_chunks.append({
                                "content": chunk_data["content"],
                                "source": doc_name,
                                "chunk": chunk_data["chunk"]
                            })
                        
                        print(f"–í—ã–±—Ä–∞–Ω–æ {len(selected_chunks)} –∫–ª—é—á–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ {total_chunks} –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc_name}'")
                    else:
                        # Fallback: –µ—Å–ª–∏ –∫—ç—à –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω
                        print(f"–ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è '{doc_name}', –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                        doc_chunks = []
                        for doc_item in self.documents:
                            if doc_item.metadata.get("source") == doc_name:
                                doc_chunks.append({
                                    "content": doc_item.page_content,
                                    "source": doc_name,
                                    "chunk": doc_item.metadata.get("chunk", 0)
                                })
                        doc_chunks.sort(key=lambda x: x['chunk'])
                        all_chunks.extend(doc_chunks)
                        self._doc_chunks_cache[doc_name] = [
                            {"content": c["content"], "chunk": c["chunk"]} 
                            for c in doc_chunks
                        ]
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É –∏ –Ω–æ–º–µ—Ä—É —á–∞–Ω–∫–∞
                all_chunks.sort(key=lambda x: (x['source'], x['chunk']))
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
                context_parts = []
                current_length = 0
                chunks_added = 0
                
                for chunk in all_chunks:
                    if chunk['chunk'] == 0:
                        fragment = f"[–ù–ê–ß–ê–õ–û –î–û–ö–£–ú–ï–ù–¢–ê '{chunk['source']}']\n{chunk['content']}"
                    else:
                        fragment = f"[–ß–∞–Ω–∫ {chunk['chunk']} –∏–∑ '{chunk['source']}']\n{chunk['content']}"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
                    if current_length + len(fragment) > max_context_length:
                        print(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã ({max_context_length} —Å–∏–º–≤–æ–ª–æ–≤). –î–æ–±–∞–≤–ª–µ–Ω–æ {chunks_added} –∏–∑ {len(all_chunks)} —á–∞–Ω–∫–æ–≤.")
                        break
                    
                    context_parts.append(fragment)
                    current_length += len(fragment)
                    chunks_added += 1
                
                context = "\n\n".join(context_parts) + "\n\n"
                
                elapsed_time = time.time() - start_time
                print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed_time:.2f}—Å: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤, {chunks_added}/{len(all_chunks)} —á–∞–Ω–∫–æ–≤")
                
            else:
                # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –†–ï–ñ–ò–ú: –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã + –Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º k –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
                k = max(k, 8)  # –ú–∏–Ω–∏–º—É–º 8 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
                docs = await self.query_documents_async(query, k=k)
                print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(docs) if isinstance(docs, list) else '–æ—à–∏–±–∫–∞'}")
                
                if isinstance(docs, str):
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {docs}")
                    return None
                
                # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                doc_names_found = set()
                for doc in docs:
                    if isinstance(doc, dict) and 'source' in doc:
                        doc_names_found.add(doc['source'])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                context_parts = []
                added_chunks = set()
                
                # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                for doc_name in doc_names_found:
                    if doc_name in self._doc_chunks_cache and self._doc_chunks_cache[doc_name]:
                        first_chunk = self._doc_chunks_cache[doc_name][0]
                        chunk_key = (doc_name, first_chunk['chunk'])
                        if chunk_key not in added_chunks:
                            context_parts.append(f"[–ù–ê–ß–ê–õ–û –î–û–ö–£–ú–ï–ù–¢–ê '{doc_name}']\n{first_chunk['content']}")
                            added_chunks.add(chunk_key)
                
                # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                for doc in docs:
                    chunk_key = (doc['source'], doc['chunk'])
                    if chunk_key not in added_chunks:
                        context_parts.append(f"–§—Ä–∞–≥–º–µ–Ω—Ç (–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc['source']}', —á–∞–Ω–∫ {doc['chunk']}):\n{doc['content']}")
                        added_chunks.add(chunk_key)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                context = "\n\n".join(context_parts)
                if len(context) > max_context_length:
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –Ω–∞—á–∞–ª–æ
                    context = context[:max_context_length]
                    context += "\n\n[–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏]"
                
                context += "\n\n"
                
                elapsed_time = time.time() - start_time
                print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed_time:.2f}—Å: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤, {len(context_parts)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            return context
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def get_document_context(self, query, k=2, include_all_chunks=None, max_context_length=30000):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è get_document_context_async (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        try:
            loop = asyncio.get_running_loop()
            # –ï—Å–ª–∏ loop –∑–∞–ø—É—â–µ–Ω, —ç—Ç–æ –æ—à–∏–±–∫–∞ - –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å async –≤–µ—Ä—Å–∏—é
            logger.error("get_document_context –≤—ã–∑–≤–∞–Ω –∏–∑ async –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ get_document_context_async()")
            return None
        except RuntimeError:
            # –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ loop, –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.run
            return asyncio.run(self.get_document_context_async(query, k, include_all_chunks, max_context_length))
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
